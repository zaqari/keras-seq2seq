{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the seq2seq Training Algorithm Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Input Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Creating a dictionary & generating embeddings\n",
    "This process is handled entirely via the lexemes module. Many downstream tasks will require using either embeddings or the dictionary from them, so we'll start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.lexemes import *\n",
    "\n",
    "#c5-3 is updated local2 in classic\n",
    "df_train = pd.read_csv('PATH/TO/FILE.csv')\n",
    "\n",
    "#Using vec model 1, building encoder embeddings\n",
    "lex = lexemes(with_nonce=True, lemmatization=True)\n",
    "f2id = lex.id_dic(df_all, ['tref', 'lex', 'head', 'lex.dep', 'lex.head'])\n",
    "f_vecs, f_err = lex.embeds(f2id)\n",
    "\n",
    "#Creating target values (from cmsource) and decoder embeddings (t_vecs)\n",
    "tar = lexemes(with_nonce=True, lemmatization=True)\n",
    "t2id = tar.id_dic(df_all, ['head', 'tref', 'tref.dep'])\n",
    "t_vecs, t_err = tar.embeds(t2id, dimensions=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Converting text to indeces using lexeme dictionaries\n",
    "We have our dictionaries, so let's now pre-process our input text. The files listed above are organized such that decoder inputs are in rows in certain columns, and encoder inputs are organized vertically in certain columns. This was merely the easiest way to keep allthe data organized at the time, but for future work the inputs module below can handle items in rows or columns as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.inputs import *\n",
    "encoder, decoder = inputs(f2id), inputs(t2id)\n",
    "encX, decX, Y = encoder.bulk_vertical(df_all, columns=['lex']), decoder.bulk_horizontal(df_all,columns=['head', 'tref.dep', 'tref']), decoder.bulk_decoderY(df_all,columns=['head', 'tref.dep', 'tref'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model Generation\n",
    "The seq2seq model module is kerasNN. You can swap it out for any other file you'd like. The .py merely needs to contain the model you're looking for, and in fact one could circumvent it entirely and just write their own code, on the fly, and then pass it to the training module. It matters not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.kerasNN import *\n",
    "#Start by initializing a model object.\n",
    "m = net(trainable_embeds=True, embeddings_list=[[f2id, 300, f_vecs],[t2id, 300, t_vecs]], decoder_initial_step=nonce, rnn_units=300)\n",
    "\n",
    "#The kerasNN module has two pre-loaded seq2seq models-- the base (b5), and experimental (e1)\n",
    "#The number of inputs will dictate the number of inputs required in pre-processing.\n",
    "m.e1(inputs=['lex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training\n",
    "Training is handled via the training module. To use it, initialize it with the model you're going to be training, the encoder data, the decoder data, and the target values built in inputs. A representations model is not required, but can be added later via train.reps = /MODEL/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq.modelHub import *\n",
    "gym = hub(m.nn1, encX, decX, Y, m.nn_rep)\n",
    "gym.train(epochs=20, cutoff=.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have validation data, add it in the .train function as a list or tuple, tagged to parameter \"validation_data\", organized as (validation_encoder_data, validataion_decoder_data, validation_target_values)\n",
    "\n",
    "A representations model is not required, but can be added later via train.reps = /MODEL/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
